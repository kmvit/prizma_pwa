"""
–°–µ—Ä–≤–∏—Å Perplexity AI –¥–ª—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –∏–∑ perplexy_bot –¥–ª—è PWA (user.id).
"""
import asyncio
import re
import httpx
from typing import List, Dict
from datetime import datetime

from app.config import PERPLEXITY_API_KEY, PERPLEXITY_MODEL, PERPLEXITY_ENABLED
from app.database.models import User, Answer, Question
from app.prompts.base import BasePrompts
from app.prompts.psychology import PsychologyPrompts
from app.prompts.premium_new import PremiumPromptsNew
from app.services.pdf_service import ReportGenerator

from loguru import logger


def _user_id(user: User) -> int:
    return getattr(user, 'id', None) or getattr(user, 'telegram_id', 0)


class PerplexityAIService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Perplexity AI"""

    def __init__(self):
        self.api_key = PERPLEXITY_API_KEY
        self.model = PERPLEXITY_MODEL or "sonar-pro"
        self.api_url = "https://api.perplexity.ai/chat/completions"
        if not self.api_key:
            raise ValueError("PERPLEXITY_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω")

    def _prepare_user_data(self, user: User, questions: List[Question], answers: List[Answer]) -> str:
        qa_pairs = []
        answer_dict = {ans.question_id: ans for ans in answers}
        for question in questions:
            answer = answer_dict.get(question.id)
            if answer and answer.text_answer:
                qa_pairs.append(f"–í–æ–ø—Ä–æ—Å {question.order_number}: {question.text}\n–û—Ç–≤–µ—Ç: {answer.text_answer}")
        return "\n\n".join(qa_pairs)

    def _get_page_specific_prompt(self, page_type: str) -> str:
        prompts_map = PsychologyPrompts.get_context_prompts_map()
        if page_type not in prompts_map:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_type}")
        return prompts_map[page_type]()

    async def _make_api_request(self, messages: List[Dict], retry_count: int = 3, is_premium: bool = False) -> Dict:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "User-Agent": "PRIZMA-AI-Psychologist/1.0"
        }
        max_tokens = 12000 if is_premium else 4000
        payload = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": 0.6,
            "stream": False
        }
        for attempt in range(retry_count):
            try:
                async with httpx.AsyncClient(timeout=120.0) as client:
                    response = await client.post(self.api_url, headers=headers, json=payload)
                if response.status_code != 200:
                    if response.status_code == 429:
                        wait_time = (2 ** attempt) * 10
                        logger.warning(f"Rate limit, –∂–¥–µ–º {wait_time}—Å")
                        await asyncio.sleep(wait_time)
                        continue
                    raise Exception(f"API Error {response.status_code}: {response.text}")
                result = response.json()
                if "choices" in result and len(result["choices"]) > 0:
                    content = result["choices"][0]["message"]["content"]
                    usage = result.get("usage", {})
                    return {"content": content, "usage": usage}
                raise Exception("–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç API")
            except (httpx.RemoteProtocolError, httpx.ReadTimeout, httpx.ConnectError) as e:
                wait_time = (2 ** attempt) * 5
                logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retry_count} –Ω–µ—É–¥–∞—á–Ω–∞: {e}")
                if attempt < retry_count - 1:
                    await asyncio.sleep(wait_time)
                else:
                    raise
        raise Exception("–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã")

    async def analyze_user_responses(self, user: User, questions: List[Question], answers: List[Answer]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —á–µ—Ä–µ–∑ Perplexity AI"""
        user_data = self._prepare_user_data(user, questions, answers)
        uid = _user_id(user)
        logger.info(f"–ó–∞–ø—É—Å–∫–∞–µ–º AI –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {uid}")

        conversation = [
            {"role": "system", "content": BasePrompts.get_common_context()},
            {"role": "user", "content": f"""–í–æ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:

{user_data}

–ò–∑—É—á–∏—Ç–µ —ç—Ç–∏ –æ—Ç–≤–µ—Ç—ã –∏ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∞–Ω–∞–ª–∏–∑–∞.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
- –í–°–ï —Ü–∏—Ç–∞—Ç—ã ‚Äî —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–æ—Å–ª–æ–≤–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –æ—Ç–≤–µ—Ç–æ–≤ –≤—ã—à–µ
- –ó–ê–ü–†–ï–©–ï–ù–û –≤—ã–¥—É–º—ã–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã
- –û–±—Ä–∞—â–∞–π—Ç–µ—Å—å –∫ —á–µ–ª–æ–≤–µ–∫—É —á–µ—Ä–µ–∑ "–í–´", "–í–ê–®–ò", "–í–ê–ú"."""
             }
        ]

        initial_response = await self._make_api_request(conversation)
        conversation.append({"role": "assistant", "content": initial_response["content"]})

        results = {}
        page_names = {"page3": "–¢–∏–ø –ª–∏—á–Ω–æ—Å—Ç–∏", "page4": "–ú—ã—à–ª–µ–Ω–∏–µ –∏ —Ä–µ—à–µ–Ω–∏—è", "page5": "–û–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"}

        for i, page_type in enumerate(["page3", "page4", "page5"]):
            page_prompt = self._get_page_specific_prompt(page_type)
            conversation.append({"role": "user", "content": page_prompt})
            page_response = await self._make_api_request(conversation)
            results[page_type] = page_response
            conversation.append({"role": "assistant", "content": page_response["content"]})
            if i < 2:
                await asyncio.sleep(3)

        return {
            "success": True,
            "page3_analysis": results["page3"]["content"],
            "page4_analysis": results["page4"]["content"],
            "page5_analysis": results["page5"]["content"],
            "usage": {},
            "timestamp": datetime.utcnow().isoformat()
        }

    def _get_section_prompt(self, section_key: str) -> str:
        methods = {
            "premium_analysis": PremiumPromptsNew.get_premium_analysis_prompt,
            "premium_strengths": PremiumPromptsNew.get_premium_strengths_prompt,
            "premium_growth_zones": PremiumPromptsNew.get_premium_growth_zones_prompt,
            "premium_compensation": PremiumPromptsNew.get_premium_compensation_prompt,
            "premium_interaction": PremiumPromptsNew.get_premium_interaction_prompt,
            "premium_prognosis": PremiumPromptsNew.get_premium_prognosis_prompt,
            "premium_practical": PremiumPromptsNew.get_premium_practical_prompt,
            "premium_conclusion": PremiumPromptsNew.get_premium_conclusion_prompt,
            "premium_appendix": PremiumPromptsNew.get_premium_appendix_prompt,
        }
        if section_key not in methods:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–∞–∑–¥–µ–ª: {section_key}")
        return methods[section_key]()

    def _extract_page_count_from_description(self, description: str) -> float:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –ø–æ–¥–±–ª–æ–∫–∞"""
        patterns = [
            r'\((\d+(?:[,\.]\d+)?)\s*—Å—Ç—Ä–∞–Ω–∏—Ü?\w*\)',
            r'\((\d+)-(\d+)\s*—Å—Ç—Ä–∞–Ω–∏—Ü?\w*\)',
        ]
        for pattern in patterns:
            match = re.search(pattern, description)
            if match:
                if len(match.groups()) == 1:
                    return float(match.group(1).replace(',', '.'))
                return (float(match.group(1)) + float(match.group(2))) / 2
        return 1.0

    def _get_premium_page_prompt(self, section_key: str, page_num: int, total_pages: int):
        """–ü—Ä–æ–º–ø—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–µ–º–∏—É–º-–∞–Ω–∞–ª–∏–∑–∞. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–ø—Ä–æ–º–ø—Ç, expected_pages)."""
        section_subblocks = {
            "premium_analysis": {"name": "–ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ—Ä—Ç—Ä–µ—Ç", "subblocks": [
                "–ê–ù–ê–õ–ò–ó BIG FIVE (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¢–ò–ü–ê MBTI (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–ê–†–•–ï–¢–ò–ü–ò–ß–ï–°–ö–ê–Ø –°–¢–†–£–ö–¢–£–†–ê (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)",
                "–ö–û–ì–ù–ò–¢–ò–í–ù–´–ô –ü–†–û–§–ò–õ–¨ (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–´–ô –ò–ù–¢–ï–õ–õ–ï–ö–¢ (1-2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)", "–°–ò–°–¢–ï–ú–ê –¶–ï–ù–ù–û–°–¢–ï–ô (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)",
                "–ö–û–ú–ú–£–ù–ò–ö–ê–¢–ò–í–ù–´–ô –°–¢–ò–õ–¨ (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–ú–û–¢–ò–í–ê–¶–ò–û–ù–ù–´–ï –î–†–ê–ô–í–ï–†–´ (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)",
                "–¢–ï–ù–ï–í–´–ï –ê–°–ü–ï–ö–¢–´ –õ–ò–ß–ù–û–°–¢–ò (1-2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)", "–≠–ö–ó–ò–°–¢–ï–ù–¶–ò–ê–õ–¨–ù–ê–Ø –ò–°–ü–û–õ–ù–ï–ù–ù–û–°–¢–¨ (1-2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)"
            ]},
            "premium_strengths": {"name": "–°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∏ —Ç–∞–ª–∞–Ω—Ç—ã", "subblocks": [
                "–ü–†–ò–†–û–î–ù–´–ï –¢–ê–õ–ê–ù–¢–´ (1,5 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)", "–ü–†–ï–î–†–ê–°–ü–û–õ–û–ñ–ï–ù–ù–û–°–¢–ò –ö –û–ü–†–ï–î–ï–õ–Å–ù–ù–´–ú –û–ë–õ–ê–°–¢–Ø–ú (0,5 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)",
                "–ü–†–ò–û–ë–†–ï–¢–Å–ù–ù–´–ï –ö–û–ú–ü–ï–¢–ï–ù–¶–ò–ò (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)", "–†–ï–°–£–†–°–ù–´–ï –°–û–°–¢–û–Ø–ù–ò–Ø (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)",
                "–ü–û–¢–ï–ù–¶–ò–ê–õ –†–ê–ó–í–ò–¢–ò–Ø (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–£–ù–ò–ö–ê–õ–¨–ù–´–ï –ö–û–ú–ë–ò–ù–ê–¶–ò–ò –ö–ê–ß–ï–°–¢–í (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)"
            ]},
            "premium_growth_zones": {"name": "–ó–æ–Ω—ã —Ä–æ—Å—Ç–∞", "subblocks": [
                "–û–ì–†–ê–ù–ò–ß–ò–í–ê–Æ–©–ò–ï –£–ë–ï–ñ–î–ï–ù–ò–Ø (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–¢–†–ê–ù–°–§–û–†–ú–ê–¶–ò–Ø –£–ë–ï–ñ–î–ï–ù–ò–ô (0.5 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)",
                "–ö–û–ì–ù–ò–¢–ò–í–ù–´–ï –ò–°–ö–ê–ñ–ï–ù–ò–Ø (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–°–õ–ï–ü–´–ï –ó–û–ù–´ (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)",
                "–≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–´–ï –¢–†–ò–ì–ì–ï–†–´ (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)", "–ü–ê–¢–¢–ï–†–ù–´ –°–ê–ú–û–°–ê–ë–û–¢–ê–ñ–ê (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)",
                "–°–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–Ø–í–õ–ï–ù–ò–Ø (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)"
            ]},
            "premium_compensation": {"name": "–ö–æ–º–ø–µ–Ω—Å–∞—Ç–æ—Ä–∏–∫–∞", "subblocks": [
                "–°–¢–†–ê–¢–ï–ì–ò–ò –†–ê–ó–í–ò–¢–ò–Ø (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)", "–¢–ï–•–ù–ò–ö–ò –°–ê–ú–û–†–ï–ì–£–õ–Ø–¶–ò–ò (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)",
                "–ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ù–´–ï –ú–û–î–ï–õ–ò –ü–û–í–ï–î–ï–ù–ò–Ø (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–†–ï–°–£–†–°–´ –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–Ø (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)",
                "–ò–ù–î–ò–í–ò–î–£–ê–õ–¨–ù–´–ô –ü–õ–ê–ù –†–ê–ó–í–ò–¢–ò–Ø (3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)", "–†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –ü–†–ê–ö–¢–ò–ö–ò (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)",
                "–û–ë–†–ê–ó–ù–û-–°–ò–ú–í–û–õ–ò–ß–ï–°–ö–ê–Ø –†–ê–ë–û–¢–ê (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)"
            ]},
            "premium_interaction": {"name": "–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ–∫—Ä—É–∂–∞—é—â–∏–º–∏", "subblocks": [
                "–°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–¨ (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–°–¢–†–ê–¢–ï–ì–ò–ò –î–õ–Ø –°–õ–û–ñ–ù–´–• –°–û–ß–ï–¢–ê–ù–ò–ô (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)",
                "–ü–ï–†–°–û–ù–ê–õ–¨–ù–´–ô –°–¢–ò–õ–¨ –û–ë–©–ï–ù–ò–Ø (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–¢–ï–•–ù–ò–ö–ò –ê–î–ê–ü–¢–ò–í–ù–û–ô –ö–û–ú–ú–£–ù–ò–ö–ê–¶–ò–ò (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)",
                "–†–û–õ–¨ –í –ö–û–ú–ê–ù–î–ï (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–ë–õ–ò–ó–ö–ò–ï –û–¢–ù–û–®–ï–ù–ò–Ø (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)",
                "–†–ê–ó–†–ï–®–ï–ù–ò–ï –ö–û–ù–§–õ–ò–ö–¢–û–í (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–°–ï–ú–ï–ô–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´ –ò –ì–†–ê–ù–ò–¶–´ (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)"
            ]},
            "premium_prognosis": {"name": "–ü—Ä–æ–≥–Ω–æ—Å—Ç–∏–∫–∞", "subblocks": [
                "–î–í–£–•–°–¶–ï–ù–ê–†–ù–´–ô –ü–†–û–ì–ù–û–ó –†–ê–ó–í–ò–¢–ò–Ø (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–ö–†–ò–ó–ò–°–´ –ò –¢–û–ß–ö–ò –†–û–°–¢–ê (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)",
                "–°–ê–ú–û–†–ï–ê–õ–ò–ó–ê–¶–ò–Ø (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–ü–†–û–ì–ù–û–ó –†–ê–ó–í–ò–¢–ò–Ø –ö–ê–ß–ï–°–¢–í (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)",
                "–î–û–õ–ì–û–°–†–û–ß–ù–´–ï –ü–ï–†–°–ü–ï–ö–¢–ò–í–´ (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–ò–¢–û–ì–û–í–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)"
            ]},
            "premium_practical": {"name": "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "subblocks": [
                "–ü–†–û–§–†–ï–ê–õ–ò–ó–ê–¶–ò–Ø (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)", "–ü–†–û–î–£–ö–¢–ò–í–ù–û–°–¢–¨ (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)", "–ü–†–ò–ù–Ø–¢–ò–ï –†–ï–®–ï–ù–ò–ô (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)",
                "–°–û–¶–ò–ê–õ–¨–ù–´–ï –ù–ê–í–´–ö–ò (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)", "–ó–î–û–†–û–í–¨–ï –ò –ë–õ–ê–ì–û–ü–û–õ–£–ß–ò–ï (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)",
                "–¢–ï–•–ù–ò–ö–ò –î–õ–Ø –°–ò–õ–¨–ù–´–• –°–¢–û–†–û–ù (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–£–ü–†–ê–ñ–ù–ï–ù–ò–Ø –î–õ–Ø –ó–û–ù –†–û–°–¢–ê (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)",
                "–ß–ï–ö-–õ–ò–°–¢–´ –ò –¢–†–ï–ö–ï–†–´ (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)"
            ]},
            "premium_conclusion": {"name": "–ó–∞–∫–ª—é—á–µ–Ω–∏–µ", "subblocks": [
                "–û–ë–û–ë–©–ï–ù–ò–ï –ò–ù–°–ê–ô–¢–û–í (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–°–ò–ù–¢–ï–ó –°–ò–õ–¨–ù–´–• –°–¢–û–†–û–ù (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)",
                "–ú–û–¢–ò–í–ê–¶–ò–û–ù–ù–û–ï –ü–û–°–õ–ê–ù–ò–ï (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)",
                "–≠–í–û–õ–Æ–¶–ò–Ø –õ–ò–ß–ù–û–°–¢–ò (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–§–ò–ù–ê–õ–¨–ù–û–ï –ü–û–°–õ–ê–ù–ò–ï (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)"
            ]},
            "premium_appendix": {"name": "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è", "subblocks": [
                "–ì–õ–û–°–°–ê–†–ò–ô –¢–ï–†–ú–ò–ù–û–í (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –†–ï–°–£–†–°–´ (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)",
                "–í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –ò –î–ò–ê–ì–†–ê–ú–ú–´ (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–ü–ï–†–°–û–ù–ê–õ–¨–ù–´–ï –ê–§–§–ò–†–ú–ê–¶–ò–ò (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)",
                "–®–ê–ë–õ–û–ù–´ –î–õ–Ø –°–ê–ú–û–ê–ù–ê–õ–ò–ó–ê (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)", "–ü–†–û–ï–ö–¢–ò–í–ù–´–ï –û–ë–†–ê–ó–´ –ò –ú–ï–¢–ê–§–û–†–´ (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)"
            ]},
        }
        if section_key not in section_subblocks or page_num > len(section_subblocks[section_key]["subblocks"]):
            raise ValueError(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} –≤ —Ä–∞–∑–¥–µ–ª–µ {section_key}")
        subblock_desc = section_subblocks[section_key]["subblocks"][page_num - 1]
        expected_pages = self._extract_page_count_from_description(subblock_desc)
        expected_chars = int(expected_pages * 3000)
        prompt = f"""–°–æ–∑–¥–∞–π—Ç–µ –°–¢–†–ê–ù–ò–¶–£ {page_num} –∏–∑ {total_pages}.
üéØ –°–û–î–ï–†–ñ–ê–ù–ò–ï: {subblock_desc}
üö® –û–ë–™–Å–ú: –¢–û–ß–ù–û {expected_chars} —Å–∏–º–≤–æ–ª–æ–≤ (¬±100 –º–∞–∫—Å–∏–º—É–º)
üéØ –¢–†–ï–ë–û–í–ê–ù–ò–Ø: –û–±—Ä–∞—â–µ–Ω–∏–µ —á–µ—Ä–µ–∑ "–í–´", "–í–ê–®–ò", "–í–ê–ú". –ú–∞–∫—Å–∏–º—É–º 2-3 —Ü–∏—Ç–∞—Ç—ã –∏–∑ –æ—Ç–≤–µ—Ç–æ–≤. –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {expected_chars} ¬± 100 —Å–∏–º–≤–æ–ª–æ–≤.
–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Ä–∞–∑–¥–µ–ª–∞."""
        return prompt, expected_pages

    def _estimate_token_count(self, text: str) -> int:
        return len(text) // 3

    def _estimate_conversation_tokens(self, conversation: List[Dict]) -> int:
        total_chars = sum(len(m.get("content", "")) for m in conversation)
        return total_chars // 3

    def _trim_conversation_context(self, conversation: List[Dict], max_tokens: int = 30000) -> List[Dict]:
        if self._estimate_conversation_tokens(conversation) <= max_tokens or len(conversation) <= 3:
            return conversation
        preserved = conversation[:3]
        remaining = conversation[3:]
        pairs = []
        for i in range(0, len(remaining), 2):
            if i + 1 < len(remaining):
                pair = [remaining[i], remaining[i + 1]]
                pair_tokens = self._estimate_token_count(pair[0].get("content", "") + pair[1].get("content", ""))
                pairs.append((pair, pair_tokens))
        kept, current_size = [], self._estimate_conversation_tokens(preserved)
        for pair, pair_tokens in reversed(pairs):
            if current_size + pair_tokens <= max_tokens:
                kept.insert(0, pair)
                current_size += pair_tokens
            else:
                break
        result = preserved[:]
        for pair in kept:
            result.extend(pair)
        logger.info(f"–£—Ä–µ–∑–∞–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ {len(kept)} –ø–∞—Ä")
        return result

    async def analyze_premium_responses(self, user: User, questions: List[Question], answers: List[Answer]) -> Dict:
        """–ü–ª–∞—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (50 –≤–æ–ø—Ä–æ—Å–æ–≤) ‚Äî –ü–û–°–¢–†–ê–ù–ò–ß–ù–ê–Ø –ì–ï–ù–ï–†–ê–¶–ò–Ø 63 —Å—Ç—Ä–∞–Ω–∏—Ü."""
        user_data = self._prepare_user_data(user, questions, answers)
        uid = _user_id(user)
        logger.info(f"–ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω—ã–π –ø—Ä–µ–º–∏—É–º AI –∞–Ω–∞–ª–∏–∑ –¥–ª—è user_id={uid} (63 —Å—Ç—Ä–∞–Ω–∏—Ü—ã)")
        conversation = [
            {"role": "system", "content": f"""{PremiumPromptsNew.get_base_prompt()}
–í–ê–ñ–ù–û: –≠—Ç–æ –±–∞–∑–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ –±—É–¥—É—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏."""},
            {"role": "user", "content": f"""–í–æ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ü–õ–ê–¢–ù–û–ì–û –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (50 –≤–æ–ø—Ä–æ—Å–æ–≤):

{user_data}

–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∏–∑—É—á–∏—Ç–µ —ç—Ç–∏ –æ—Ç–≤–µ—Ç—ã –∏ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–∏—é –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –ü–õ–ê–¢–ù–û–ì–û –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.

üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –¶–ò–¢–ê–¢–´:
- –í–°–ï —Ü–∏—Ç–∞—Ç—ã –∏ –ø—Ä–∏–º–µ—Ä—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¢–û–õ–¨–ö–û —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–æ—Å–ª–æ–≤–Ω—ã–º–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏ –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –≤—ã—à–µ
- –ó–ê–ü–†–ï–©–ï–ù–û –≤—ã–¥—É–º—ã–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∏–ª–∏ —Ü–∏—Ç–∞—Ç—ã
- –ï—Å–ª–∏ –≤ –æ—Ç–≤–µ—Ç–∞—Ö –Ω–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–π —Ü–∏—Ç–∞—Ç—ã - –ù–ï —Å–æ–∑–¥–∞–≤–∞–π—Ç–µ –ø—Ä–∏–º–µ—Ä
- –û–±—Ä–∞—â–∞–π—Ç–µ—Å—å –∫ —á–µ–ª–æ–≤–µ–∫—É —á–µ—Ä–µ–∑ "–í–´", "–í–ê–®–ò", "–í–ê–ú" - –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–ª–æ–≤–∞ "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å" –∏–ª–∏ "–∫–ª–∏–µ–Ω—Ç"."""}
        ]
        initial_response = await self._make_api_request(conversation, is_premium=True)
        conversation.append({"role": "assistant", "content": initial_response["content"]})
        logger.info(f"–ü–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ–ª—É—á–µ–Ω: {len(initial_response['content'])} —Å–∏–º–≤–æ–ª–æ–≤")

        page_structure = [
            ("premium_analysis", "–ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ—Ä—Ç—Ä–µ—Ç", 10),
            ("premium_strengths", "–°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∏ —Ç–∞–ª–∞–Ω—Ç—ã", 5),
            ("premium_growth_zones", "–ó–æ–Ω—ã —Ä–æ—Å—Ç–∞", 7),
            ("premium_compensation", "–ö–æ–º–ø–µ–Ω—Å–∞—Ç–æ—Ä–∏–∫–∞", 7),
            ("premium_interaction", "–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ–∫—Ä—É–∂–∞—é—â–∏–º–∏", 8),
            ("premium_prognosis", "–ü—Ä–æ–≥–Ω–æ—Å—Ç–∏–∫–∞", 6),
            ("premium_practical", "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", 8),
            ("premium_conclusion", "–ó–∞–∫–ª—é—á–µ–Ω–∏–µ", 6),
            ("premium_appendix", "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è", 6)
        ]
        all_pages = {}
        all_individual_pages = {}
        page_counter = 1

        for section_key, section_name, page_count in page_structure:
            logger.info(f"–†–∞–∑–¥–µ–ª: {section_name} ({page_count} —Å—Ç—Ä–∞–Ω–∏—Ü)")
            section_prompt = self._get_section_prompt(section_key)
            conversation.append({"role": "user", "content": f'–ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Ä–∞–∑–¥–µ–ª—É "{section_name}". –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:\n{section_prompt}\n–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞.'})
            section_response = await self._make_api_request(conversation, is_premium=True)
            conversation.append({"role": "assistant", "content": section_response["content"]})

            for page_num in range(1, page_count + 1):
                current_tokens = self._estimate_conversation_tokens(conversation)
                if current_tokens > 50000:
                    conversation = self._trim_conversation_context(conversation, max_tokens=30000)

                page_prompt, _ = self._get_premium_page_prompt(section_key, page_num, page_count)
                conversation.append({"role": "user", "content": page_prompt})
                page_response = await self._make_api_request(conversation, is_premium=True)

                all_individual_pages[f"page_{page_counter:02d}"] = {
                    "content": page_response["content"],
                    "section": section_name,
                    "section_key": section_key,
                    "page_num": page_num,
                    "global_page": page_counter
                }
                conversation.append({"role": "assistant", "content": page_response["content"]})
                page_counter += 1
                await asyncio.sleep(1)

            section_pages = {k: v["content"] for k, v in all_individual_pages.items() if v["section_key"] == section_key}
            all_pages[section_key] = "\n\n".join(section_pages.values())

        total_length = sum(len(c) for c in all_pages.values())
        logger.info(f"–ü—Ä–µ–º–∏—É–º-–∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω: {total_length} —Å–∏–º–≤–æ–ª–æ–≤, 63 —Å—Ç—Ä–∞–Ω–∏—Ü—ã")

        return {
            "success": True,
            "premium_analysis": all_pages.get("premium_analysis", ""),
            "premium_strengths": all_pages.get("premium_strengths", ""),
            "premium_growth_zones": all_pages.get("premium_growth_zones", ""),
            "premium_compensation": all_pages.get("premium_compensation", ""),
            "premium_interaction": all_pages.get("premium_interaction", ""),
            "premium_prognosis": all_pages.get("premium_prognosis", ""),
            "premium_practical": all_pages.get("premium_practical", ""),
            "premium_conclusion": all_pages.get("premium_conclusion", ""),
            "premium_appendix": all_pages.get("premium_appendix", ""),
            "individual_pages": all_individual_pages,
            "initial_analysis": initial_response["content"],
            "usage": {"pages_generated": 63},
            "timestamp": datetime.utcnow().isoformat()
        }


def _create_fallback_analysis() -> Dict:
    """–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ –ò–ò"""
    return {
        "success": True,
        "page3_analysis": "–í–∞—à–∏ –æ—Ç–≤–µ—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –ª–∏—á–Ω–æ—Å—Ç–∏. –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –ø—Ä–∏ –≤–∫–ª—é—á–µ–Ω–∏–∏ Perplexity API.",
        "page4_analysis": "–ù–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—à–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–∂–Ω–æ –≤—ã–¥–µ–ª–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤–∞—à–µ–≥–æ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è.",
        "page5_analysis": "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é –±—É–¥—É—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –æ—Ç—á–µ—Ç–∞.",
        "usage": {},
        "timestamp": datetime.utcnow().isoformat()
    }


class AIAnalysisService:
    """
    –§–∞—Å–∞–¥ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç—á—ë—Ç–∞.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Perplexity AI –ø—Ä–∏ PERPLEXITY_ENABLED=true, –∏–Ω–∞—á–µ ‚Äî fallback + PDF.
    """
    def __init__(self):
        self.perplexity_enabled = PERPLEXITY_ENABLED
        self.ai_service = PerplexityAIService() if (PERPLEXITY_ENABLED and PERPLEXITY_API_KEY) else None
        self.report_generator = ReportGenerator()

    async def generate_psychological_report(
        self, user: User, questions: List[Question], answers: List[Answer]
    ) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –æ—Ç—á–µ—Ç–∞ (PDF)"""
        try:
            if self.perplexity_enabled and self.ai_service:
                analysis_result = await self.ai_service.analyze_user_responses(user, questions, answers)
                if not analysis_result.get("success"):
                    analysis_result = _create_fallback_analysis()
            else:
                analysis_result = _create_fallback_analysis()

            report_filepath = self.report_generator.create_pdf_report(user, analysis_result)
            logger.info(f"–û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {report_filepath}")

            return {
                "success": True,
                "report_file": report_filepath,
                "page3_analysis": analysis_result.get("page3_analysis", ""),
                "page4_analysis": analysis_result.get("page4_analysis", ""),
                "page5_analysis": analysis_result.get("page5_analysis", ""),
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")
            return {
                "success": False,
                "error": str(e),
                "stage": "general"
            }

    async def generate_premium_report(
        self, user: User, questions: List[Question], answers: List[Answer]
    ) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–º–∏—É–º-–æ—Ç—á—ë—Ç–∞ (template_pdf_premium)"""
        try:
            if self.perplexity_enabled and self.ai_service:
                analysis_result = await self.ai_service.analyze_premium_responses(user, questions, answers)
                if not analysis_result.get("success"):
                    raise Exception(analysis_result.get("error", "AI error"))
            else:
                raise Exception("–ü—Ä–µ–º–∏—É–º-–æ—Ç—á—ë—Ç —Ç—Ä–µ–±—É–µ—Ç PERPLEXITY_ENABLED")
            report_filepath = self.report_generator.create_premium_pdf_report(user, analysis_result)
            logger.info(f"–ü—Ä–µ–º–∏—É–º-–æ—Ç—á—ë—Ç —Å–æ–∑–¥–∞–Ω: {report_filepath}")
            return {"success": True, "report_file": report_filepath}
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–µ–º–∏—É–º-–æ—Ç—á—ë—Ç–∞: {e}")
            return {"success": False, "error": str(e), "stage": "premium"}
